===========================================================Fine-Tuning DistilBERT with PEFT + LoRA on Rotten TomatoesModel: distilbert-base-uncasedTechnique: LoRA (Low-Rank Adaptation)Task: 3-Class Sentiment ClassificationSTEP 1: Before Training (Untrained Model Predictions)It was good.                                 -> NeutralNot a fan, don't recommend.                  -> PositiveBetter than the first one.                   -> NeutralThis is not worth watching even once.        -> NeutralThis one is a pass.                          -> PositiveSTEP 2: Training Summary• Epochs: 3  • Batch Size: 16  • Learning Rate: 2e-4  • LoRA Rank (r): 8  • LoRA Alpha: 16  • LoRA Dropout: 0.1  • Parameters Tuned: ~0.5% of model total  • Training Time: ~2–3 mins (on Colab T4 GPU)STEP 3: After Training (Fine-Tuned Model Predictions)It was good.                                 -> PositiveNot a fan, don't recommend.                  -> NegativeBetter than the first one.                   -> PositiveThis is not worth watching even once.        -> NegativeThis one is a pass.                          -> NegativeSTEP 4: Observations - Sentiment predictions became more accurate after LoRA fine-tuning.   - Model successfully learned to differentiate positive and negative tone.   - Fine-tuning achieved using minimal compute (parameter-efficient).===========================================================